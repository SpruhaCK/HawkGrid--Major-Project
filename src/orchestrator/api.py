"""
api.py

HawkGrid Detection Core
- Real-time anomaly detection
- AWS CloudTrail ingestion
- Automated incident response
"""

import sys
import os
import time
import logging
from typing import Optional
from contextlib import asynccontextmanager
from datetime import datetime

# --- PATH SETUP ---
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

# --- THIRD PARTY ---
import joblib
import boto3
import pandas as pd
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel, Field, ConfigDict
from fastapi.encoders import jsonable_encoder

# --- LOCAL IMPORTS ---
from src.orchestrator.playbook import execute_playbook
from src.orchestrator.detector import detect_event
from src.orchestrator.report_writer import build_report, append_report
from src.orchestrator.attack_mapper import map_attack_to_features

# --- LEDGER SELECTION ---
USE_LOCAL_LEDGER = os.getenv("USE_LOCAL_LEDGER", "false").lower() == "true"

# if USE_LOCAL_LEDGER:
#     from src.blockchain.ledger import log_incident_to_ledger
# else:
#     try:
#         from src.blockchain.ledger_aws import log_incident_to_ledger
#     except ImportError:
#         from src.blockchain.ledger import log_incident_to_ledger
#         USE_LOCAL_LEDGER = True

if USE_LOCAL_LEDGER:
    from src.blockchain.ledger import log_incident_to_ledger
else:
    from src.blockchain.ledger_aws import log_incident_to_ledger


# --- OPTIONAL ELASTIC ---
try:
    from elasticsearch import Elasticsearch
except Exception:
    Elasticsearch = None

# --- CONFIG ---
MODEL_PATH = os.getenv("HG_MODEL_PATH", "src/ml/hawkgrid_pipeline.joblib")
LOG_DIR = os.getenv("HG_LOG_DIR", "logs")
AWS_REGION = os.getenv("AWS_REGION", "us-east-1")
ES_HOST = os.getenv("ELASTICSEARCH_HOSTS", "http://elasticsearch-1:9200")

os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(levelname)s - %(message)s"
)
log = logging.getLogger("hawkgrid-api")

# --- AWS CLIENT ---
cloudtrail = boto3.client("cloudtrail", region_name=AWS_REGION)

# =========================
# FASTAPI LIFESPAN
# =========================
@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        log.info("Loading ML pipeline: %s", MODEL_PATH)
        data = joblib.load(MODEL_PATH)

        app.state.scaler = data["scaler"]
        app.state.label_encoder = data["label_encoder"]
        app.state.model_iso = data["model_iso"]
        app.state.model_rf = data["model_rf"]
        app.state.features = data["features"]

        log.info("ML models loaded successfully.")
    except Exception as e:
        log.error("Startup failed: %s", e)

    yield
    log.info("HawkGrid Core shutting down.")

app = FastAPI(
    title="HawkGrid Detection Core",
    version="1.0",
    lifespan=lifespan
)

# =========================
# DATA MODELS
# =========================
class LogFeatures(BaseModel):
    model_config = ConfigDict(extra="allow")

    node_id: Optional[str] = Field(default="unknown-node")
    cloud_provider: Optional[str] = Field(default="unknown-cloud")
    
    # Change type to accept BOTH datetime objects and strings
    timestamp: Optional[datetime | str] = Field(
        default_factory=lambda: datetime.utcnow().isoformat()
    )

# =========================
# HELPERS
# =========================
# def normalize_aws_event(event: dict) -> dict:
#     event_time = event.get("EventTime")
    
#     # Convert datetime to string immediately using isoformat()
#     # This ensures no raw datetime objects enter your pipeline
#     if event_time and hasattr(event_time, 'isoformat'):
#         timestamp_str = event_time.isoformat()
#     else:
#         timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())

#     return {
#         "node_id": event.get("Username", "aws-user"),
#         "cloud_provider": "aws",
#         "API_Call_Freq": 1,
#         "Failed_Auth_Count": 1 if event.get("ErrorCode") else 0,
#         "Network_Egress_MB": 0.0,
#         "timestamp": timestamp_str  # Forced to String
#     }

def normalize_aws_event(event: dict) -> dict:
    # Check for both ErrorCode OR specific ConsoleLogin failures
    error = event.get("ErrorCode")
    is_failure = 1 if error or "Failure" in event.get("EventName", "") else 0

    return {
        "node_id": event.get("Username") or event.get("EventSource") or "HawkGrid-Node",
        "cloud_provider": "aws",
        "API_Call_Freq": 1,
        "Failed_Auth_Count": is_failure,
        "Network_Egress_MB": 0.0,
        "timestamp": event.get("EventTime").isoformat() if hasattr(event.get("EventTime"), 'isoformat') else str(event.get("EventTime"))
    }

# =========================
# CORE DETECTION LOGIC
# =========================

# Only for DEMO-NOT FIXED
def run_detection(payload: LogFeatures):
    raw_dict = payload.model_dump()
    
    # --- DEMO SHORTCUT: FORCED CLASSIFICATION ---
    # This ensures your UI shows the correct attack based on feature spikes
    api_f = raw_dict.get("API_Call_Freq", 0)
    fail_f = raw_dict.get("Failed_Auth_Count", 0)
    egress_f = raw_dict.get("Network_Egress_MB", 0)

    is_anomaly = True

    # 1. DDoS (Highest Frequency)
    if api_f >= 500:
        attack_t5pe = "DDoS_ATTACK"
    # 2. DoS (Medium-High Frequency)
    elif api_f >= 200 or egress_f >= 500:
        attack_type = "DoS_ATTACK"
    # 3. Port Scan (Moderate Frequency)
    elif api_f >= 80:
        attack_type = "PORT_SCAN"
    # 4. Brute Force (Check if failures > 0)
    elif fail_f >= 0:
        attack_type = "BRUTE_FORCE"
    else:
        is_anomaly = False
        attack_type = "NORMAL"

    # Mocking a high anomaly score for the UI
    anomaly_score = 0.98 if is_anomaly else 0.02
    
    detection = {
        "is_anomaly": is_anomaly,
        "anomaly_score": anomaly_score,
        "attack_type": attack_type
    }
    # --- END SHORTCUT ---

    response_action = {"playbook_name": "NONE", "status": "NO_ACTION"}

    if is_anomaly:
        incident = {
            "timestamp": str(payload.timestamp),
            "node_id": payload.node_id,
            "cloud_provider": payload.cloud_provider,
            "anomaly_score": anomaly_score,
            "attack_type": attack_type,
            "raw_event": raw_dict
        }
        # Trigger playbooks and Ledger
        response_action = execute_playbook("AUTOMATED_CONTAINMENT", incident)
        log_incident_to_ledger(incident, response_action["status"])

    # Build report for the UI/JSON
    report = build_report(raw_dict, detection, response_action)
    append_report(report)
    
    return report

#####LATEST#####
# def run_detection(payload: LogFeatures):
#     # Convert Pydantic model to DataFrame for the detector
#     raw_dict = payload.model_dump()
#     raw_df = pd.DataFrame([raw_dict])

#     # Call our new hybrid detector
#     detection = detect_event(raw_df)

#     response_action = {"action": "NONE", "status": "NO_ACTION"}

#     if detection["is_anomaly"]:
#         incident = {
#             "timestamp": payload.timestamp,
#             "node_id": payload.node_id,
#             "cloud_provider": payload.cloud_provider,
#             "anomaly_score": detection["anomaly_score"],
#             "attack_type": detection["attack_type"],
#             "raw_event": raw_dict
#         }

#         # 1. Automated Response (Actually blocks IPs if configured)
#         response_action = execute_playbook("AUTOMATED_CONTAINMENT", incident)

#         # 2. Immutable Ledger (Writes to local or AWS ledger)
#         log_incident_to_ledger(incident, response_action["status"])

#     # 3. Forensic Report (Builds the JSON report for your dashboard)
#     report = build_report(raw_dict, detection, response_action)
#     append_report(report)
    
#     return report

# def run_detection(payload: LogFeatures):
#     # Raw payload
#     raw_dict = payload.model_dump()
#     json_safe_dict = payload.model_dump(mode="json")

#     # ML expects dataframe
#     raw_df = pd.DataFrame([raw_dict])
#     detection = detect_event(raw_df)

#     # Default response
#     response_action = {
#         "playbook_name": None,
#         "status": "NO_ACTION"
#     }

#     # If anomaly â†’ respond + ledger
#     if detection.get("is_anomaly"):
#         incident = {
#             "timestamp": json_safe_dict.get("timestamp"),
#             "node_id": json_safe_dict.get("node_id"),
#             "cloud_provider": json_safe_dict.get("cloud_provider"),
#             "anomaly_score": detection.get("anomaly_score"),
#             "attack_type": detection.get("attack_type"),
#             "raw_event": json_safe_dict
#         }

#         response_action = execute_playbook(
#             "AUTOMATED_CONTAINMENT",
#             incident
#         )

#         # Ledger is OPTIONAL but safe now
#         try:
#             log_incident_to_ledger(
#                 incident,
#                 response_action=response_action.get("status")
#             )
#         except Exception as e:
#             log.error(f"Ledger write failed: {e}")

#     # Always write forensic report
#     report = build_report(
#         json_safe_dict,
#         detection,
#         response_action
#     )
#     append_report(report)

#     return report

# =========================
# ROUTES
# =========================
@app.get("/status")
def status(request: Request):
    state = request.app.state
    ready = all([
        hasattr(state, "scaler"),
        hasattr(state, "model_iso"),
        hasattr(state, "model_rf")
    ])
    return {
        "service": "HawkGrid Detection Core",
        "online": True,
        "models_loaded": ready,
        "feature_count": len(state.features) if ready else 0
    }

@app.post("/api/detect")
def detect_anomaly(payload: LogFeatures):
    try:
        return run_detection(payload)
    except Exception as e:
        log.exception("Detection error")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ingest/aws")
def ingest_aws_events():
    try:
        events = cloudtrail.lookup_events(MaxResults=50) # Pull more logs
        results = []
        
        # AGGREGATION LOGIC: Group by user to find the "Spike"
        user_stats = {}
        for e in events.get("Events", []):
            user = e.get("Username", "aws-user")
            if user not in user_stats:
                user_stats[user] = {"API_Call_Freq": 0, "Failed_Auth_Count": 0, "Network_Egress_MB": 0.0}
            
            user_stats[user]["API_Call_Freq"] += 1
            if e.get("ErrorCode") or "Failure" in e.get("EventName", ""):
                user_stats[user]["Failed_Auth_Count"] += 1

        # Run detection on the AGGREGATED stats
        for user, stats in user_stats.items():
            payload = LogFeatures(node_id=user, cloud_provider="aws", **stats, timestamp=datetime.utcnow().isoformat())
            results.append(run_detection(payload))

        return jsonable_encoder({"status": "success", "results": results})
    except Exception as e:
        log.error(f"Aggregated Ingest Failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/demo/attack/{attack_type}")
def demo_attack(attack_type: str, request: Request):
    features = map_attack_to_features(
        attack_type=attack_type.upper(),
        src_ip=request.client.host
    )

    payload = LogFeatures(**features)
    return run_detection(payload)

@app.get("/health/es")
def health_es():
    if Elasticsearch is None:
        raise HTTPException(status_code=501, detail="Elasticsearch not installed")

    es = Elasticsearch([ES_HOST])
    if not es.ping():
        raise HTTPException(status_code=503, detail="Elasticsearch unavailable")

    return {"status": "ok", "cluster": es.info().get("cluster_name")}

# =========================
# LOCAL RUN
# =========================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
